---
title: "Data Processing in R and Python assignment 1"
author: "Erdni Mankirov"
date: "12 12 2022"
output: html_document
---



## Introduction
The main purpoise of the project is to rewrite quries that was used to extract some data from tables which were downloaded from  https://travel.stackexchange.com/ . The following data frames wased used:
• Badges.csv.gz
• Comments.csv.gz
• Posts.csv.gz
• Users.csv.gz
• Votes.csv.gz
With the help of 'sqldf` package I will check the SQL statements from the tasks and with the help of bases functions of R, `dplyr` and `data.table` packages, I will rewrite this quries and compare all of them.
Lets load the data and packages then.

```{r setup, message = FALSE, warning = FALSE}
library(sqldf)
library(dplyr)
library(data.table)
knitr::opts_chunk$set(cache = TRUE)
knitr::opts_knit$set(root.dir = "~/pd1/travel_stackexchange_com")
options(stringsAsFactors = FALSE)
Sys.setenv(TZ = "UTC")
```



```{r data}
Badges <- read.csv("C:\\Users\\user\\Desktop\\Badges.csv")
Posts <- read.csv("C:\\Users\\user\\Desktop\\Posts.csv")
Votes <- read.csv("C:\\Users\\user\\Desktop\\Votes.csv.gz")
Users <- read.csv("C:\\Users\\user\\Desktop\\Users.csv.gz")
Comments <- read.csv("C:\\Users\\user\\Desktop\\Comments.csv.gz")
```
I wiil use the following funtions to compare the quries (the second function was created only for 5th quory which I did not manage to do completely):
```{r Everytghingequiv, dependson = "data"}

Everytghingequiv <- function(sqldftest, basetest, dplyrtest, dataTabletest) {
  result <- all(
    compare::compare(sqldftest, basetest, allowAll = TRUE)$result,
    compare::compare(sqldftest, as.data.frame(dplyrtest), allowAll = TRUE)$result,
    compare::compare(sqldftest, as.data.frame(dataTabletest), allowAll = TRUE)$result
  )
  return(result)
}

Everytghingequiv5 <- function(sqldftest,dplyrtest, dataTabletest) {
  result <- all(
    compare::compare(sqldftest, as.data.frame(dataTabletest), allowAll = TRUE)$result,
    compare::compare(sqldftest, as.data.frame(dplyrtest), allowAll = TRUE)$result
   
  )
  return(result)
}
```


## First query

The first query just counts the number of posts by year.
```{r sqldf_1, dependson = "data"}
sqldf_1 <- function(Posts){
  sqldf1 <- sqldf("SELECT STRFTIME('%Y', CreationDate) AS Year, COUNT(*) AS TotalNumber
FROM Posts
GROUP BY Year
")
  return(sqldf1)
}
sqldf_1(Posts)
```
The following queries perform the query above. In first one I used lapply function to get the year from the CreationDate,mutate and format was used with the same purpose in dplyr and datatable. Then I used aggregate to group it by year and count the posts' ids by it's year. In dplyr will be used group by and summarize for the same purpoise. 

```{r base_1, dependson = "data"}
base_1 <- function(Posts){
  years1<-lapply(Posts["CreationDate"], substr, 1, 4)
  count1<-aggregate(Posts$Id, by = years1, FUN = length )
  colnames(count1)<- c("Year","TotalNumber")
  return (count1)
}
base_1(Posts)
```

```{r dplyr_1, dependson = "data"}
dplyr_1 <- function(Posts){
  dployer1 <- Posts %>%
    mutate(Year = substr(CreationDate, 1, 4)) %>%
    group_by(Year) %>%
    summarize(TotalNumber = length(Id)) 
  return(dployer1)
}
dplyr_1(Posts)
```

```{r data.table_1, dependson = "data"}
data.table_1 <- function(Posts){
  postst <- as.data.table(Posts)
  Years <- postst[, .(Id, Year = format(
    as.POSIXct(CreationDate, format = "%Y-%m-%d"),
    "%Y"))
  ]
  dataTable1 <- Years[, .(TotalNumber = length(Id)),by = Year]
  return(dataTable1)
}
data.table_1(Posts)
```
Let's Compare this functions:
```{r check1, dependson = c("Everytghingequiv", "sqldf_1", "base_1", "dplyr_1", "data.table_1")}
Everytghingequiv(sqldf_1(Posts), base_1(Posts), dplyr_1(Posts), data.table_1(Posts))
```

Additionally, I measure the time it takes to evaluate every implementation of the query above. As we can see dplyr has the fastest perfomance while sqldf the slowest one.  In this paricular xase base functions are facter than data table.

```{r extime1,dependson = c(sqldf_1", "base_1", "dplyr_1", "data.table_1")}
microbenchmark::microbenchmark(sqldf = sqldf_1(Posts), base = base_1(Posts),dplyr = dplyr_1(Posts), dataTable = data.table_1(Posts), times = 1)
```


## Second query

The second query has the subquery that find ViewCount with PostTypeId=1  and then groups by Id the sum of the Top10 users with the hight viewCount sum.
```{r sqldf_2, dependson = "data"}
sqldf_2 <- function(Users, Posts){
  sqldf2 <- sqldf("SELECT Id, DisplayName, SUM(ViewCount) AS TotalViews
FROM Users
JOIN (
SELECT OwnerUserId, ViewCount FROM Posts WHERE PostTypeId = 1
) AS Questions
ON Users.Id = Questions.OwnerUserId
GROUP BY Id
ORDER BY TotalViews DESC
LIMIT 10
")
  return(sqldf2)
}
sqldf_2(Users, Posts)
```
In the base function firstly I extract data from the Posts and filtering it by PostTypeId according to subquery. Then I use merge as analog of join and extract the columns that are needed from new big table.
And finally I aggregate the data to fund sum of the ViewCount by Id.

```{r base_2, dependson = "data"}
base_2 <- function(Users, Posts){
  tmpt<-Posts[Posts$PostTypeId==1, c('OwnerUserId', 'ViewCount')]
  base2 <-merge(Users, tmpt, by.x = "Id", by.y = "OwnerUserId")
  base2 <-base2[c('Id', 'DisplayName','ViewCount')]
  base2 <- aggregate(ViewCount~Id+DisplayName,base2,FUN = sum)
  base2 <- head(base2[order(base2$ViewCount, decreasing = TRUE),], 10)
  base2 <- setNames(base2, c("Id","DisplayName", "TotalViews"))
  return(base2)
}
base_2(Users, Posts)

```
The same operatins I do here but with different function. Join = inner_join(),
where= filter(), group by = group_by(), aggregate()= summarize(), Order by = arrange(), Limit = slice_head()
```{r dplyr_2, dependson = "data"}
dplyr_2 <- function(Users, Posts){
dplyr2 <- Users %>%
    inner_join(Posts, by = c("Id" = "OwnerUserId")) %>%
    filter(PostTypeId == 1) %>%
    group_by(Id) %>%
    summarize(TotalViews = sum(ViewCount)) %>%
    arrange(desc(TotalViews)) %>%
    slice_head(n = 10) 
    return(dplyr2)
}
dplyr_2(Users, Posts)
```
Firstly I set keys for bith tables that will be used , then I used the inner function head the will slice first 10 rows,  first brackets and nomatc = 0 equivalent to inner join, and in the next brackets I filter the data and sum the TotalViews by Id and order by TotalViews DECS.
```{r data.table_2, dependson = "data"}
data.table_2 <- function(Users, Posts){
  Users <- as.data.table(Users)
  setkey(Users, "Id")
  Posts <- as.data.table(Posts)
  setkey(Posts, OwnerUserId)
  dataTable2 <- head(Users[Posts, nomatch = 0
  ][PostTypeId == 1, .(TotalViews= sum(ViewCount)), by = Id
  ][order(-TotalViews)
  ], 10)
  return(dataTable2)
}
data.table_2(Users, Posts)

```
The comparsion of this function will give us False because I did not manage to select DisplayName column in dplyr and datatable. But the rest of the columns are equal:
```{r check2, dependson = c("Everytghingequiv", "sqldf_2", "base_2", "dplyr_2", "data.table_2")}
Everytghingequiv(sqldf_2(Users, Posts), base_2(Users, Posts), dplyr_2(Users, Posts), data.table_2(Users, Posts))
```

Here We can  see that sqldf is still slowest one, while dplyr is fastest. But know dataTable performs nearly as fast as dplyr. 
```{r extime2,dependson = c(" "sqldf_2", "base_2", "dplyr_2", "data.table_2")}
microbenchmark::microbenchmark(sqldf = sqldf_2(Users, Posts), base = base_2(Users, Posts),dplyr = dplyr_2(Users, Posts), dataTable = data.table_2(Users, Posts), times = 1)
```

## Third query

The third query finds the author with highst percentage of the Badges in each year.
```{r sqldf_3, dependson = "data"}
sqldf_3 <- function(Badges){
  sqldf3 <- sqldf("SELECT Year, Name, MAX((Count * 1.0) / CountTotal) AS MaxPercentage
FROM (
SELECT BadgesNames.Year, BadgesNames.Name, BadgesNames.Count, BadgesYearly.CountTotal
FROM (
SELECT Name, COUNT(*) AS Count, STRFTIME('%Y', Badges.Date) AS Year
FROM Badges
GROUP BY Name, Year
) AS BadgesNames
JOIN (
SELECT COUNT(*) AS CountTotal, STRFTIME('%Y', Badges.Date) AS Year
FROM Badges
GROUP BY YEAR
) AS BadgesYearly
ON BadgesNames.Year = BadgesYearly.Year
)
GROUP BY Year
")
return(sqldf3)
}
sqldf_3(Badges)
```
In the base function firstly I found the years to aggregate Badges by it in subquery , then I found the table BadgesNames that is grouped by years1 (same as years but not list) and names, I merge it in one bigtable and then aggregate it by years with the help of max function and formula that was used to calculate percentage of badges counts. Then I tried to merge the table with names but failed. At least it shows years and max percentage.

```{r base_3, dependson = "data"}
base_3 <- function(Badges){
  years<-lapply(Badges["Date"], substr, 1, 4)
  BadgesYearly<-aggregate(Badges$Id, by = years, FUN = length )
  
  Time <- as.Date(Badges$Date)
  years1<-as.numeric(format(Time,'%Y'))
  BadgesYearly<-BadgesYearly[,c(2,1)]
  colnames(BadgesYearly)<- c("CountTotal","Year")
  BadgesNames<-aggregate(Badges$Id ~ years1 + Badges$Name,data=Badges, FUN = length )
  BadgesNames<-BadgesNames[,c(2,3,1)]
  colnames(BadgesNames)<- c("Name","Count","Year")
  bigjoin <-merge(BadgesNames, BadgesYearly, by.x = "Year", by.y = "Year")
  
  tmp1 <-aggregate(bigjoin$Count*1.0/bigjoin$CountTotal,by = list(bigjoin$Year),FUN = max)
  colnames(tmp1)<- c("Year","MaxPercentage")
  tmp2 <- bigjoin[bigjoin$Year %in% tmp1$Year,c("Year", "Name")]
  base3 <- merge(tmp2, tmp1)
  return(tmp1)
}
base_3(Badges)

```
Here I made the BadgesYearly and BadgesNames tables separately , as they are pretty similar I used only mutate() to extract the year and hroup by with summarize like in previous one. Then I join both tables in bigtable and select only 4 needed columns. Finally I group by and summarize by MaxPercentage.
```{r dplyr_3, dependson = "data"}
dplyr_3 <- function(Badges){
  BadgesYearly <- Badges %>%
    mutate(Year = substr(Date, 1, 4)) %>%
    group_by(Year) %>%
    summarize(CountTotal = length(Id))
  BadgesNames<-Badges %>%
    mutate(Year = substr(Date, 1, 4)) %>%
    group_by(Name,Year) %>%
    summarize(Count = length(Id))
  bigtable <- BadgesYearly %>%
    inner_join(BadgesNames, by = c("Year" = "Year")) %>%
    select(Year, Name, Count, CountTotal)
  
  dplyr3<-bigtable %>%
    group_by(Year) %>%
    summarise(MaxPercentage = max((Count * 1.0) / CountTotal))
  return(dplyr3)
}

dplyr_3(Badges)
```
Here I first of all extract the year for creating the separate tables BadgesYearly and BadgesName.  Then I join this 2 tables   bigtable1<-BadgesName[BadgesYearly, on = .(Year)]
and group by year the MaxPercentage.
```{r data.table_3, dependson = "data"}

data.table_3 <- function(Badges){
  badgest <- as.data.table(Badges)
  Years <- badgest[, .(Id, Year = format(
    as.POSIXct(Date, format = "%Y-%m-%d"),
    "%Y"))]
  BadgesYearly <- Years[, .(CountTotal = length(Id)),by = Year]
  Years2 <- badgest[, .(Id,Name, Year = format( as.POSIXct(Date, format = "%Y-%m-%d"),"%Y"))]
  BadgesName <- Years2[, .(Count = length(Id)),by = list(Name,Year)]
  bigtable1<-BadgesName[BadgesYearly, on = .(Year)]
  dataTable3<-bigtable1[,list(MaxPercentage=max((Count * 1.0) / CountTotal)), by=Year]
  return(dataTable3)
}

data.table_3(Badges)

```
The output is again false because I did not manage to extract the Name column but MaxPercentage and Year was calculated and grouped correctly in all functions. 
```{r check3, dependson = c("Everytghingequiv", "sqldf_3", "base_3", "dplyr_3", "data.table_3")}
Everytghingequiv(sqldf_3(Badges), base_3(Badges), dplyr_3(Badges), data.table_3(Badges))
```

Here the datable has the worst perfomance, even worser than sqldf. dplyr is still the queckiest one. 
```{r extime3,dependson = c( "sqldf_3", "base_3", "dplyr_3", "data.table_3")}
microbenchmark::microbenchmark(sqldf = sqldf_3(Badges), base = base_3(Badges), dplyr = dplyr_3(Badges), dataTable = data.table_3(Badges), times = 1)

```

## Fourth query

In the Fourth query we just extract a lot of data from the big table that was created by joins of subqueries. First subquery is needed just to get CommentTotalScore  and the second subquery creates the table with comments totatl score that are filtered by  PostTypeId=1. Then in the  last join we just select lots of columns from new big table. And select the TOP10 rows with highest CommentsTotalScore.
```{r sqldf_4, dependson = "data"}
sqldf_4 <- function(Comments, Posts, Users){
  sqldf4 <- sqldf("SELECT Title, CommentCount, ViewCount, CommentsTotalScore, DisplayName, Reputation, Location
FROM (
SELECT Posts.OwnerUserId, Posts.Title, Posts.CommentCount, Posts.ViewCount,
CmtTotScr.CommentsTotalScore
FROM (
SELECT PostId, SUM(Score) AS CommentsTotalScore
FROM Comments
GROUP BY PostId
) AS CmtTotScr
JOIN Posts ON Posts.Id = CmtTotScr.PostId
WHERE Posts.PostTypeId=1
) AS PostsBestComments
JOIN Users ON PostsBestComments.OwnerUserId = Users.Id
ORDER BY CommentsTotalScore DESC
LIMIT 10
")
  return(sqldf4)
}
sqldf_4(Comments, Posts, Users)
```
This task was probably easiest one. Firstly I extract the required data from Comments table and create CommentTotalScore column by agregation. Then we just merge(join) the tables by id and select the required columns [c(...)] . In the end I just select top 10 Commentswith highest total score by head[,10] and order functions.

```{r base_4, dependson = "data"}
base_4 <- function(Comments, Posts, Users){
  CmtTotScr<-Comments["PostId"]
  CmtTotScr<-aggregate(Comments$Score, by = list(Comments$PostId), FUN = sum )
  colnames(CmtTotScr)<- c("PostId","CommentsTotalScore")
  
  PostsBestComments<-merge(CmtTotScr, Posts[Posts["PostTypeId"] == 1,],by.x = "PostId", by.y = "Id")[c("OwnerUserId", "Title", "CommentCount","ViewCount","CommentsTotalScore")]
  base4<-merge(PostsBestComments,Users,by.x = "OwnerUserId",by.y = "Id")[c("Title", "CommentCount", "ViewCount","CommentsTotalScore","DisplayName","Reputation","Location")]
  base4 <- head(base4[order(base4$CommentsTotalScore, decreasing = TRUE),], 10)
  return(base4)
}
base_4(Comments, Posts, Users)

```

Here I  extract the data from comments and summarized Score to get Comments totatal score for each comment, table is  grouped by PostId data . Then I just join it with Posts and filter by PostsTypeId==1, and select the needed columns. Then the last join selection of the needed columns , arrangment by DECS and slice the top 10.  
```{r dplyr_4, dependson = "data"}
dplyr_4 <- function(Comments, Posts, Users){
  CmtTotScr <- Comments %>%
    group_by(PostId) %>%
    summarize(CommentsTotalScore = sum(Score))
  PostsBestComments<-  CmtTotScr %>%
    inner_join(Posts, by = c("PostId" = "Id")) %>%
    filter(PostTypeId == 1) %>%
    select(OwnerUserId, Title, CommentCount, ViewCount,CommentsTotalScore) 
  
  
  dplyr4 <- PostsBestComments %>%
    inner_join(Users, by = c("OwnerUserId" = "Id")) %>%
    select(Title, CommentCount, ViewCount, CommentsTotalScore, DisplayName, Reputation, Location)%>%
    arrange(desc(CommentsTotalScore)) %>%
    slice_head(n = 10)
  return(dplyr4)
}

dplyr_4(Comments, Posts, Users)
```
  CmtTotScr<-commentst[,.(CommentsTotalScore=sum(Score)),by=PostId] creating the first table from subquery. Pretty obvious. 
 
  PostsBestComments<-CmtTotScr[postst, nomatch = 0][PostTypeId == 1, .(OwnerUserId, Title, CommentCount, ViewCount,CommentsTotalScore)] Joining the tables and filtering by PostTypeId, then select the needed columns.
  In final line we do the same things but add also ordering by decreasing  of CommentsTotalScore [order(-CommentsTotalScore)] . Finally slice the first 10.
```{r data.table_4, dependson = "data"}

data.table_4 <- function(Comments, Posts, Users){
  postst <- as.data.table(Posts)
  commentst <- as.data.table(Comments)
  userst <- as.data.table(Users)
  CmtTotScr<-commentst[,.(CommentsTotalScore=sum(Score)),by=PostId]
  setkey(CmtTotScr, PostId)
  setkey(postst, Id)
  setkey(userst, Id)
  
  PostsBestComments<-CmtTotScr[postst, nomatch = 0][PostTypeId == 1, .(OwnerUserId, Title, CommentCount, ViewCount,CommentsTotalScore)]
  setkey(PostsBestComments, OwnerUserId)
  dataTable4 <-PostsBestComments[userst, nomatch = 0][,.(Title, CommentCount, ViewCount, CommentsTotalScore, DisplayName, Reputation, Location)][order(-CommentsTotalScore)]
  dataTable4 <- head(dataTable4,10)
  return(dataTable4)
}


data.table_4(Comments, Posts, Users))

```
Output is true. 
```{r check4, dependson = c("Everytghingequiv", "sqldf_4", "base_4", "dplyr_4", "data.table_4")}
Everytghingequiv(sqldf_4(Comments, Posts, Users),base_4(Comments, Posts, Users),dplyr_4(Comments, Posts, Users),data.table_4(Comments, Posts, Users))
```
In this query the data table has the best perfomance, As expected sqldf is slowest one.
```{r extime4,dependson = c( "sqldf_4", "base_4", "dplyr_4", "data.table_4")}
microbenchmark::microbenchmark(sqldf = sqldf_4(Comments, Posts, Users), base = base_4(Comments, Posts, Users), dplyr = dplyr_4(Comments, Posts, Users), dataTable = data.table_4(Comments, Posts, Users), times = 1)

```


## Fifth query

The Fifth query we want to see top 20 posts with the highest upvotes During COVID years. 
```{r sqldf_5, dependson = "data"}

sqldf_5 <- function(Posts, Votes){
  sqldf5 <- sqldf("SELECT Posts.Title, STRFTIME('%Y-%m-%d', Posts.CreationDate) AS Date, VotesByAge.*
FROM Posts
JOIN (
SELECT PostId,
MAX(CASE WHEN VoteDate = 'before' THEN Total ELSE 0 END) BeforeCOVIDVotes,
MAX(CASE WHEN VoteDate = 'during' THEN Total ELSE 0 END) DuringCOVIDVotes,
MAX(CASE WHEN VoteDate = 'after' THEN Total ELSE 0 END) AfterCOVIDVotes,
SUM(Total) AS Votes
FROM (
SELECT PostId,
CASE STRFTIME('%Y', CreationDate)
WHEN '2022' THEN 'after'
WHEN '2021' THEN 'during'
WHEN '2020' THEN 'during'
WHEN '2019' THEN 'during'
ELSE 'before'
END VoteDate, COUNT(*) AS Total
FROM votes
WHERE VoteTypeId IN (3, 4, 12)
GROUP BY PostId, VoteDate
) AS VotesDates
GROUP BY VotesDates.PostId
) AS VotesByAge ON Posts.Id = VotesByAge.PostId
WHERE Title NOT IN ('') AND DuringCOVIDVotes > 0
ORDER BY DuringCOVIDVotes DESC, votes DESC
LIMIT 20
")
  
  return(sqldf5) 
}

sqldf_5(Posts, Votes)
```

To get the VotesDate mostly I used the previous function and moves to get the year and filtering. But now also I had to write case_when function to separate votes by Covid periods. Then in VotesDate2 I summarized votes for each period.  Finally I get the full data with mutate and filter all non-empty title posts and non zero DuringCOVIDVotes. The rest is just order by and limit 20. 
```{r dplyr_5, dependson = "data"}

dplyr_5 <- function(Posts, Votes){
  VotesDates <- Votes %>%
    filter(VoteTypeId %in% c(3, 4, 12)) %>%
    mutate(VoteDate = substr(CreationDate, 1, 4)) %>%
    mutate(VoteDate = case_when(VoteDate %in% c(2021, 2020,2019) ~ "during", VoteDate %in% c(2022) ~ "after",
                                TRUE ~ "before")) %>%
    count(PostId, VoteDate, name = "Total")
  
  VotesDates2 <- VotesDates %>%
    group_by(PostId) %>%
    summarize(BeforeCOVIDVotes = max(
      case_when(VoteDate == "before" ~ Total, TRUE ~ 0L)),
      DuringCOVIDVotes = max(
        case_when(VoteDate == "during" ~ Total, TRUE ~ 0L)),
      AfterCOVIDVotes = max(
        case_when(VoteDate == "after" ~ Total, TRUE ~ 0L)), Votes= sum(Total))
  
  dplyr5 <- Posts %>%
    inner_join(VotesDates2, by = c("Id" = "PostId")) %>%
    mutate(Date = substr(CreationDate, 1, 10)) %>%
    filter(DuringCOVIDVotes > 0, !Title %in% c("") ) %>%
    select(Title, Date,PostId=Id,BeforeCOVIDVotes,DuringCOVIDVotes,AfterCOVIDVotes,Votes) %>%
    arrange(desc(DuringCOVIDVotes),desc(Votes)) %>%
    slice_head(n = 20)
  return(dplyr5)
}

dplyr_5(Posts, Votes)
```
Here I performed typical filtrations and selections but add a new function fcase to separate cases.
```{r data.table_5, dependson = "data"}

data.table_5 <- function(Posts, Votes){
  votest <- as.data.table(Votes)
  VotesDates <- votest[VoteTypeId %in% c(3, 4, 12), .(PostId, CreationDate)
  ][, .(PostId, VoteDate = fcase(substr(CreationDate, 1, 4)%in% c(2021, 2020,2019), "during", 
                                 substr(CreationDate, 1, 4)%in% c(2022), "after", default = "before" ))
  ][, .(Total = .N), by = .(PostId, VoteDate)
  ]
  
  VotesDates2 <- VotesDates[, .(PostId,
                                BeforeCOVIDVotes = ifelse(VoteDate == "before", Total, 0),
                                DuringCOVIDVotes = ifelse(VoteDate == "during", Total, 0),
                                AfterCOVIDVotes = ifelse(VoteDate == "after", Total, 0),
                                Total)
  ][, .(BeforeCOVIDVotes = max(BeforeCOVIDVotes),
        DuringCOVIDVotes = max(DuringCOVIDVotes),
        AfterCOVIDVotes = max(AfterCOVIDVotes), 
        Votes=sum(Total)),
    by = PostId
  ]
  setkey(VotesDates2, PostId)
  
  postst <- as.data.table(Posts)
  setkey(postst, Id)
  dataTable5 <- head(postst[VotesDates2
  ][DuringCOVIDVotes>0 & !Title %in% "" & !is.na(Title), .(Title, Date = format(as.POSIXct(CreationDate, format = "%Y-%m-%d"),"%Y-%m-%d"),PostId = Id,BeforeCOVIDVotes,DuringCOVIDVotes,AfterCOVIDVotes,Votes)
  ][order(-DuringCOVIDVotes,-Votes)
  ], 20)
  return(dataTable5)
}


data.table_5(Posts, Votes)

```
Output is true. 
```{r check5, dependson = c("Everytghingequiv5", "sqldf_5",  "dplyr_5", "data.table_5")}
Everytghingequiv5(sqldf_5(Posts, Votes),dplyr_5(Posts, Votes),data.table_5(Posts, Votes))

```
And the last query surprised us with the slowest dplyr performance and MUCH faster datatable calculations.

```{r extime5,dependson = c( "sqldf_5",  "dplyr_5", "data.table_5")}
microbenchmark::microbenchmark(sqldf = sqldf_5(Posts, Votes),  dplyr = dplyr_5(Posts, Votes), dataTable = data.table_5(Posts, Votes), times = 1)


```


## Conclusion
In conclusion we can say that for different queires the performance of dplyr,SQLdf,data table and basic R functions are also different.  So We can say that for each task we should not use the same approach.


source('Erdni_Mankirov_assignment_1.R')
